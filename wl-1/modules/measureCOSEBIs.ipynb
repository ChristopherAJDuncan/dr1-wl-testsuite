{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bc0e7a",
   "metadata": {},
   "source": [
    "# Measuring COSEBIs from Shear Catalogs\n",
    "\n",
    "Author: Laila Linke (laila.linke@uibk.ac.at)\n",
    "\n",
    "This notebook demonstrates how to measure COSEBIs (Complete Orthogonal Sets of E/B-Integrals) from weak lensing shear catalogs. The workflow includes:\n",
    "\n",
    "- Reading in galaxy shear data from a FITS catalog (as is available from cosmohub).\n",
    "- Selecting galaxies based on user specified cuts.\n",
    "- Measuring shear correlation functions (ξ₊, ξ₋) for all tomographic bin combinations using TreeCorr.\n",
    "- Saving the correlation functions for each bin combination.\n",
    "- Running an external COSEBIs pipeline to compute COSEBIs modes from the measured correlation functions.\n",
    "\n",
    "Adjust file paths, column names, and parameters as needed for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a56478",
   "metadata": {},
   "source": [
    "This code relies on `treecorr` by Mike Jarvis and the COSEBI part of `kcap` by the KiDS team, in particular Marika Asgari.\n",
    "\n",
    "Before running it, you need to install `treecorr`, using\n",
    "\n",
    "`pip install treecorr`\n",
    "\n",
    "and download/clone the folder `cosebis` from `https://github.com/KiDS-WL/kcap/tree/master/cosebis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "from astropy.table import Table\n",
    "import treecorr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b6caf",
   "metadata": {},
   "source": [
    "## 0. User specifications\n",
    "\n",
    "First you need to set several constants. These are\n",
    "- The name of the data catalog fits file\n",
    "- The output directory for shear correlation functions and COSEBIs\n",
    "- The location of the cosebi scripts from kcap\n",
    "- The location of the cosebi normalization and root lookup tables from kcap\n",
    "- The location of the kernel functions for the cosebi calculation from kcap. This folder can be empty, then the kernel functions are calculated on the fly from the normalization and roots\n",
    "- The ascii file containing the covariance\n",
    "- The number of tomo bins\n",
    "- The minimal and maximal angular scale for the correlation functions\n",
    "- The number of bins for the correlation functions (at least 1000 is recommended for COSEBIs)\n",
    "- The maximum COSEBI mode to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ab5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datacatalog = \"../../../../data/RR2_2.1/23090.fits\"\n",
    "out_directory = \"../../../../measurements/2pt_RR2_lensmc/\"\n",
    "\n",
    "bin_cosebis_directory = \"../../../cosebis/measurement_pipeline/\"\n",
    "norm_roots_directory = \"../../../cosebis/TLogsRootsAndNorms/\"\n",
    "Ts_directory=\"../../../cosebis/Tplus_minus/\"\n",
    "\n",
    "n_max_covariance=10 # number of COSEBIs modes in covariance per tomo-bin combination\n",
    "\n",
    "Ntomo = 6 # number of tomographic bins\n",
    "theta_min = 0.5 # minimum angular separation in arcmin\n",
    "theta_max = 300 # maximum angular separation in arcmin\n",
    "Nbins = 1000 # number of angular bins for correlation functions (>=1000 is recommended)\n",
    "n_max = 10 # maximum COSEBIs mode to compute\n",
    "\n",
    "covariance_file=f\"../../../../data/RR2_2.1/covariance_{theta_min}_{theta_max}_{n_max_covariance}/covariance_matrix.mat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d8670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {out_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b98061",
   "metadata": {},
   "source": [
    "## 1. Read in data and make selections\n",
    "Next, we read in the data and make selections. Here you can specify any additional selections you need and also change the column names if your data uses different ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Table.read(datacatalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35992430",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (data['she_lensmc_weight']>0)\n",
    "# Add more selections here if needed\n",
    "data = data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b38cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these column names if your data uses different ones\n",
    "ra=data['she_lensmc_ra']\n",
    "dec=data['she_lensmc_dec']\n",
    "ra_units='deg'\n",
    "dec_units='deg'\n",
    "g1=data['she_lensmc_e1_corrected']\n",
    "g2=data['she_lensmc_e2_corrected']\n",
    "w=data['she_lensmc_weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071010ef",
   "metadata": {},
   "source": [
    "## 2. Measure shear correlation functions for each tomo bin combination\n",
    "\n",
    "Now we calculate the shear correlation functions using treecorr. The results are written to an ascii file for each tomographic bin combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18629ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Ntomo):\n",
    "    print(\"Processing tomographic bin combination: \", i, i)\n",
    "    # Auto-correlations\n",
    "\n",
    "    # Mask for picking tomographic bin\n",
    "    mask_tomo=(data['tom_bin_id']==i+1)\n",
    "\n",
    "    # Create TreeCorr catalog\n",
    "    cat1 = treecorr.Catalog(ra=ra[mask_tomo], dec=dec[mask_tomo], ra_units=ra_units, \n",
    "                            dec_units=dec_units, g1=g1[mask_tomo], g2=g2[mask_tomo], w=w[mask_tomo])\n",
    "\n",
    "    # Set up the correlation function object\n",
    "    gg = treecorr.GGCorrelation(min_sep=theta_min, max_sep=theta_max, nbins=Nbins, sep_units='arcmin')\n",
    "\n",
    "    # Measure the correlation functions\n",
    "    gg.process(cat1)\n",
    "\n",
    "    # Save the correlation functions\n",
    "    np.savetxt(out_directory+\"gg_thMin\"+str(theta_min)+\"_thMax\"+str(theta_max)+\"_tomo\"+str(i)+\"_\"+str(i)+\".dat\", \n",
    "               np.column_stack([gg.meanr, gg.xip, gg.xim]))\n",
    "    \n",
    "\n",
    "    for j in range(i+1, Ntomo):\n",
    "        print(\"Processing tomographic bin combination: \", i, j)\n",
    "        # Cross-correlations\n",
    "\n",
    "        # Mask for picking tomographic bin\n",
    "        mask_tomo2=(data['tom_bin_id']==j+1)\n",
    "\n",
    "        # Create TreeCorr catalog\n",
    "        cat2 = treecorr.Catalog(ra=ra[mask_tomo2], dec=dec[mask_tomo2], ra_units=ra_units, \n",
    "                                dec_units=dec_units, g1=g1[mask_tomo2], g2=g2[mask_tomo2], w=w[mask_tomo2])\n",
    "\n",
    "        # Set up the correlation function object\n",
    "        gg = treecorr.GGCorrelation(min_sep=theta_min, max_sep=theta_max, nbins=Nbins, sep_units='arcmin')\n",
    "\n",
    "        # Measure the correlation functions\n",
    "        gg.process(cat1, cat2)\n",
    "\n",
    "        # Save the correlation functions\n",
    "        np.savetxt(out_directory+\"gg_thMin\"+str(theta_min)+\"_thMax\"+str(theta_max)+\"_tomo\"+str(i)+\"_\"+str(j)+\".dat\", \n",
    "                   np.column_stack([gg.meanr, gg.xip, gg.xim]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03442441",
   "metadata": {},
   "source": [
    "## 3. Measure COSEBIs\n",
    "Finally, we can convert the correlation functions to COSEBIs. For this we use the kcap routine which we call directly from this notebook for each tomographic bin combination. The resulting COSEBIs are written to file (one for each tomographic bin combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format theta_min and theta_max for filenames\n",
    "theta_min_str=f\"{theta_min:.2f}\"\n",
    "theta_max_str=f\"{theta_max:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e15d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute COSEBIs for each tomographic bin combination\n",
    "for i in range(Ntomo):\n",
    "    for j in range(i, Ntomo):\n",
    "        print(\"Processing COSEBIs for tomographic bin combination: \", i, j)\n",
    "\n",
    "        file=out_directory+\"/gg_thMin\"+str(theta_min)+\"_thMax\"+str(theta_max)+\"_tomo\"+str(i)+\"_\"+str(j)+\".dat\"\n",
    "        outfile=\"cosebis_thMin\"+str(theta_min_str)+\"_thMax\"+str(theta_max_str)+\"_tomo\"+str(i)+\"_\"+str(j)\n",
    "\n",
    "        !python {bin_cosebis_directory}run_measure_cosebis_cats2stats.py -i {file} --cfoldername {out_directory} -o {outfile} --norm {norms_roots_directory}/Normalization_{theta_min_str}-{theta_max_str}.table -r {norms_roots_directory}/Root_{theta_min_str}-{theta_max_str}.table -b log -s {theta_min_str} -l {theta_max_str} -n {n_max} --tfoldername {Ts_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3613a",
   "metadata": {},
   "source": [
    "# 4. Calculate $p$-value for $B$-modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39180ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in covariance\n",
    "\n",
    "Cov = np.loadtxt(covariance_file)\n",
    "\n",
    "x=len(Cov)// (2*n_max_covariance)\n",
    "\n",
    "\n",
    "Cov_reshape=Cov.reshape(2*x, n_max_covariance, 2*x, n_max_covariance)\n",
    "\n",
    "Cov_cut = Cov_reshape[:, :n_max, :, :n_max].reshape(2*x*n_max, 2*x*n_max)\n",
    "\n",
    "Cov_B = Cov_cut[x*n_max:, x*n_max:]\n",
    "\n",
    "error_B=np.sqrt(np.diag(Cov_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c92d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in B-modes\n",
    "Bn={}\n",
    "\n",
    "for i in range(Ntomo):\n",
    "    for j in range(i, Ntomo):\n",
    "        tmp=np.loadtxt(f\"{out_directory}/Bn_cosebis_thMin{theta_min_str}_thMax{theta_max_str}_tomo{i}_{j}.ascii\")\n",
    "        Bn[f\"{i}-{j}\"]=tmp\n",
    "\n",
    "\n",
    "datavector_B = np.array([Bn[key] for key in Bn]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p-values\n",
    "\n",
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_p(C, d):\n",
    "    chi2_val = d.T @ np.linalg.inv(C) @ d\n",
    "    dof = len(d)\n",
    "    #print(chi2_val, dof)\n",
    "    p_value = chi2.sf(chi2_val, dof)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7980c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall p-value\n",
    "\n",
    "p_value = get_p(Cov_B, datavector_B)\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-value for tomo  bins\n",
    "\n",
    "p_value_tomos={}\n",
    "\n",
    "for i in range(Ntomo):\n",
    "    for j in range(i, Ntomo):\n",
    "        datavector = np.array([Bn[f\"{i}-{j}\"]]).flatten()\n",
    "\n",
    "        C= Cov_B[(i+j)*n_max:(i+j+1)*n_max,(i+j)*n_max:(i+j+1)*n_max]\n",
    "\n",
    "\n",
    "        p_value_tomos[f\"{i}-{j}\"]=get_p(C, datavector)\n",
    "\n",
    "p_value_tomos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1068a2a",
   "metadata": {},
   "source": [
    "# 5. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if you do not have niceplots, comment out the following two lines\n",
    "import niceplots\n",
    "niceplots.initPlot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw, fh=plt.rcParams['figure.figsize']\n",
    "\n",
    "fig, axes=plt.subplots(ncols=Ntomo, nrows=Ntomo, figsize=(2*fw, 2*fh),\n",
    "                        sharex=True, sharey='row')\n",
    "plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "\n",
    "for i in range(Ntomo):\n",
    "    for j in range(Ntomo):\n",
    "        if i>j:\n",
    "            axes[j, i].set_axis_off()\n",
    "        else:\n",
    "            err = error_B[(i+j)*n_max:(i+j+1)*n_max]\n",
    "\n",
    "\n",
    "            axes[j, i].errorbar(range(1, n_max+1), Bn[f\"{i}-{j}\"], yerr=err, ls='', marker='x')\n",
    "            axes[j, i].axhline(0, color='grey', ls='--', zorder=10)\n",
    "\n",
    "            axes[j, i].text(5, -2e-10, f\"$p$: {p_value_tomos[f\"{i}-{j}\"]:.1e}\")\n",
    "\n",
    "\n",
    "for i in range(Ntomo):\n",
    "    axes[i, 0].set_ylabel(r'$B_n$')\n",
    "    axes[Ntomo-1, i].set_xlabel(r'$n$')\n",
    "    axes[Ntomo-1, i].set_xlim(0, n_max+2)\n",
    "    axes[i, Ntomo-1].set_ylim(-3e-10, 3e-10)\n",
    "\n",
    "\n",
    "plt.suptitle(r\"$\\theta \\in $\"+f\"[${theta_min}'$, ${theta_max}'$]\")\n",
    "\n",
    "axes[2, 4].text(0, 1e-9, f\"$p$ for $B_n$: {p_value:.1e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfffeb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c61614d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
